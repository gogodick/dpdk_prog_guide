
# 8. 轮询模式驱动
DPDK支持1G，10G，40G和半虚拟化的virtio轮询模式驱动。

轮询模式驱动(PMD)由API组成，这些API通过用户空间运行的BSD驱动提供，用于配置设备和对应的队列。此外，PMD不需要中断（除了链路状态改变中断），直接访问RX和TX的描述符，能够在用户应用快速的接收，处理和传输报文。本章描述了PMD的需求和总体设计原则，为以太网PMD提出了高层次架构和通用的外部API。

## 8.1. 需求和假设
DPDK环境对于报文处理应用支持两种模型，run-to-completion和pipe-line：

* 在run-to-completion模型里，通过API轮询指定端口的RX描述符ring。然后在同一个核上处理报文，通过传输API把报文放到端口的TX描述符ring。
* 在pipe-line模型里，1个核通过API轮询一个或者多个端口的RX描述符ring。报文通过ring传递到另一个核。另一个核继续处理报文，可能会通过传输API把报文放到端口的TX描述符ring。

在同步run-to-completion模型里，DPDK分配的逻辑核执行报文处理循环，包括以下步骤：

* 通过PMD接收API获取输入报文
* 每次处理一个接收的报文，直到转发
* 通过PMD传输API把报文发出去

相对的，在异步pipe-line模型里，一些逻辑核可能用来接收报文，而另一些逻辑核用来处理之前收到的报文。收到的报文在逻辑核之间通过ring交换。报文接收过程包括以下步骤：

* 通过PMD接收API获取输入报文
* 通过报文队列把报文发给处理lcore

报文处理包括以下步骤：

* 从报文队列获取收到的报文
* 处理收到的报文，直到重新传输

为了避免不必要的中断处理开销，执行环境不能使用任何异步通知机制。有必要时，应该尽可能使用ring来实现异步通知。

在多核环境中，避免锁竞争是关键的问题。为了解决这个问题，PMD的设计是尽可能使用每个核的私有资源。例如，如果PMD不支持DEV_TX_OFFLOAD_MT_LOCKFREE，PMD为每个port，每个核维护一个单独的传输队列。同样的，每个接收队列只分配给一个逻辑核（lcore）进行轮询。

为了和Non-Uniform Memory Access（NUMA）保持一致，内存管理的设计是为每个逻辑核在本地内存分配一个私有的内存池，从而最小化远端内存访问。报文内存池的配置应该考虑物理内存的架构，比如DIMM，通道和rank。应用必须保证内存池创建时使用合适的参数。参考[Mempool Library](https://github.com/gogodick/dpdk_prog_guide/blob/master/Text/6.md)。

## 8.2. 设计原则
以太网PMD的API和架构的设计遵循了以下原则。

PMD必须在上层应用帮助实现全局策略。相对的，网卡PMD函数不应该妨碍甚至阻止上层的全局策略。

举例来说，PMD的接收和传输函数都需要轮询大量的报文或者描述符。run-to-completion处理栈可以通过不同的全局循环策略来优化整体性能，比如：

* 每次接收，处理和传输一个报文，零碎的方式。
* 尽可能多的接收报文，然后处理全部报文，立即传输这些报文。
* 接收指定数量的报文，处理和积累这些收到的报文，最后把所有积累的报文发出去。

为了优化性能，必须考虑整体软件设计选择和纯软件优化技术，并且和底层硬件优化功能（CPU的cache属性，总线速度，网卡的PCI带宽等等）进行平衡。报文传输就是一个这样的例子，在优化突发网络报文的处理引擎时，要在软件和硬件之间权衡。对第一种情况，PMD只需要提供rte_eth_tx_one函数，每次在一个队列上传输一个报文。但是，PMD有效的实现了rte_eth_tx_burst函数，使得每个报文的驱动级别的传输代价最小化，使用了以下优化：

* 在多个报文间均分调用rte_eth_tx_one函数的未摊销代价。
* rte_eth_tx_burst函数利用了基于突发的硬件功能（预取数据到cache，使用网卡的头寄存器和尾寄存器），减少了处理每个报文的CPU周期，例如避免对ring传输描述符的没有必要的读访问，或者系统的使用匹配cache行边界和大小的指针数组。
* 使用基于突发的软件优化技术，省掉了其他情况下无法避免的操作，比如ring索引的回滚管理。

API也引入了基于突发的函数，PMD深入的使用了这些服务。特别是生成网卡ring的buffer分配器，能够一次分配和释放多个buffer。举例来说，mbuf_multiple_alloc返回一个指向rte_mbuf的指针数组，使得PMD的接收轮询函数能够在接收ring快速的安装多个描述符。

## 8.3. 逻辑核，内存和网卡队列关系
DPDK支持NUMA，当处理器的逻辑核和接口使用本地内存时，能够得到更好的性能。所以，为本地PCIE接口分配mbuf时，应该使用在本地内存创建的内存池。buffer应该在本地处理器使用，才能得到最好的性能，并且RX和TX的buffer描述符对应的mbuf应该使用在本地内存创建的内存池分配。

为了run-to-completion模型的性能更好，应该在本地内存而不是远端内存执行报文和数据的操作。对于pipe-line模型也一样，所有逻辑核都属于同样的处理器。

多个逻辑核绝不能共享接收队列和传输队列，因为需要全局锁从而影响性能。

如果PMD支持DEV_TX_OFFLOAD_MT_LOCKFREE，多个线程可以不使用软件锁，在同一个tx队列上同时调用rte_eth_tx_burst()。部分网卡支持这个PMD功能，在以下情况有用：

* 有些应用的lcore和TX队列的映射不是1：1，可以不使用spinlock。
* 对于eventdev，不需要用一个专用TX核传输，所有核都能传输报文，更加灵活

更多细节可以参考硬件能力DEV_TX_OFFLOAD_MT_LOCKFREE。

## 8.4. 设备识别和配置
### 8.4.1. 设备识别
每个网卡端口有唯一的PCI标识（总线号，设备号，功能号，BDF），是DPDK初始化执行的PCI探测和枚举函数分配的。基于PCI标识，网卡端口分配了两个其他的标识：

* 端口索引，PMD的API的所有函数使用这个索引来指定网卡端口。
* 端口名字，在控制台消息中指定端口，用于管理或者调试。为了便于使用，端口名字包含端口索引。
### 8.4.2. 设备配置
每个网卡端口配置包括以下操作：

* 分配PCI资源
* 重置设备（发送全局reset）到缺省状态
* 设置PHY和链路
* 初始化统计计数器

PMD的API必须提供函数支持开始和停止端口的多播功能，以及提供函数支持打开和关闭端口的混杂模式。

一些硬件offload功能必须在端口初始化阶段通过特定的参数配置。例如，Receive Side Scaling（RSS）和Data Center Bridging（DCB）就是这种情况。

### 8.4.3. 运行时修改配置
可以在运行时（不需要停止设备）打开或关闭的所有设备功能，不需要PMD的API提供专门的函数。

需要设备的PCI寄存器的映射地址，用来在驱动之外的特定函数实现配置这些功能。

为了这个目的，PMD的API提供了一个函数，能够提供设备有关的全部信息，可以用来在驱动之外配置指定的设备功能。包括了PCI供应商ID，PCI设备ID，PCI设备寄存器的映射地址，还有驱动的名字。

这个方式的主要优点是，可以自由的选择API来配置，开始和停止这种功能。

相应的例子可以参考testpmd应用，配置Intel的82576网卡和Intel的82599网卡的IEEE1588功能。

其他比如L3和L4的五元组的报文过滤功能，可以使用相同方式配置。以太网流控（pause帧）可以在独立端口配置。从testpmd源代码获取更多细节。另外，可以配置报文的mbuf，让网卡计算这个报文的L4（UDP，TCP和SCTP）校验和。参考硬件offload获取更多细节。

### 8.4.4. 传输队列的配置
每个传输队列的配置需要以下信息：

* 传输ring的描述符数量
* socket标识，指定NUMA架构中的合适DMA内存区域，用于分配传输ring
* 传输队列的Prefetch，Host和Write-Back阈值寄存器的值
* 最小传输报文的释放阈值（tx_free_thresh）。当传输报文使用的描述符数量超过这个阈值，网卡应该检查是否有回写描述符。配置TX队列时，0表示应该使用缺省值。tx_free_thresh的缺省值是32。这种方式保证了，网卡为队列处理了32个描述符以后，PMD才会搜索全部描述符。
* 最小RS位阈值。在设置传输描述符的Report Status（RS）位之前，使用的传输描述符的最小数量。注意这个参数只对Intel的10G网卡有效。如果从前一次设置RS位的描述符到这个报文的第一个描述符之间的描述符数量超过了传输RS位阈值（tx_rs_thresh），设置这个报文的最后一个描述符的RS位。简单的说，这个参数控制网卡把哪个传输描述符回写到主机内存。在配置TX队列时，0表示使用缺省值。tx_rs_thresh的缺省值是32。保证了在网卡回写最近使用的描述符之前，至少使用了32个描述符。这种方式节约了TX描述符回写使用的PCIE带宽。需要注意的是，当tx_rs_thresh大于1时，TX回写阈值（TX wthresh）应该设为0。参考Intel的82599的10G网卡数据表，获取更多细节。

tx_free_thresh和tx_rs_thresh必须满足以下限制：

* tx_rs_thresh必须大于0。
* tx_rs_thresh必须小于ring大小减2。
* tx_rs_thresh必须小于等于tx_free_thresh。
* tx_free_thresh必须大于0。
* tx_free_thresh必须小于ring大小减3。
* 为了优化性能，当tx_rs_thresh大于1时，TX wthresh必须设为0。

TX ring的一个描述必须作为哨兵使用，避免硬件冲突，这是最大阈值限制。

| 注意|
| :---|
|在端口初始化配置DCB操作时，传输队列和接收队列的数量必须配成128。|

### 8.4.5. 按需求释放TX的mbuf
很多驱动在传输报文后，不会立即释放mbuf到mempool或者本地cache。而是把mbuf留在TX的ring，等到超过tx_rs_thresh阈值后再批量释放，或者TX的ring需要槽位再释放mbuf。

应用可以使用rte_eth_tx_done_cleanup()的API通知驱动释放使用的mbuf。这个API通知驱动释放不再使用的mbuf，和tx_rs_thresh无关。以下是应用需要立即释放mbuf的两个场景：

* 报文需要传输到多个目的接口（二层的洪泛或者三层的组播）。一个选项是复制一份报文或者复制需要修改的报文头。另一个选项是传输报文，然后轮询rte_eth_tx_done_cleanup()这个API，直到报文的引用计数减少。然后同一个报文可以传输到下一个目的接口。应用仍然需要负责修改不同目的接口的报文，但是避免了复制报文。不管传输还是丢弃报文，API只关心接口是否还在使用mbuf。
* 一些应用需要多次运行，比如报文生成器。为了每次运行的性能和一致性，应用可能需要每次运行都重置到初始状态，释放所有mbuf到mempool。在这种情况下，应用可以对每个目的接口调用rte_eth_tx_done_cleanup()，通知释放所有mbuf。

在网卡驱动文档检查按需求释放TX的mbuf功能，确定驱动是否支持这个API。

### 8.4.6. 硬件offload
根据rte_eth_dev_info_get()得到的驱动能力，PMD可以支持硬件offload功能，比如checksum，TCP分段，插入VLAN标签或者同一个TX队列的无锁多线程发送。

支持这些offload功能，需要在rte_mbuf数据结构里设置专用的状态位和字段，并且PMD提供的接收和传输函数需要合适的处理。对于标志位和具体意义，可以参考mbuf的API文档和[Mbuf Library](https://github.com/gogodick/dpdk_prog_guide/blob/master/Text/7.md)的章节“meta信息”。

#### 8.4.6.1. 端口offload和队列offload
DPDK的offload的API，把offload划分成端口offload和队列offload。可以使用rte_eth_dev_info_get()查询不同的offload能力。可以支持端口offload或者队列offload。

使用DEV_TX_OFFLOAD_\*或者DEV_RX_OFFLOAD_\*打开offload。使用rte_eth_dev_configure配置端口offload。使用rte_eth_rx_queue_setup和rte_eth_tx_queue_setup配置队列offload。为了打开端口offload，需要配置设备和队列。在混合配置时，队列设置会返回错误。为了打开队列配置，应该只配置队列。没有打开的offload都是缺省关闭的。

应用需要使用TX的offload的API时，应该设置rte_eth_txconf结构的txq_flags字段的ETH_TXQ_FLAGS_IGNORE标志。这种情况不需要设置txq_flags的其他标志。应用需要使用RX的offload的API时，应该设置rte_eth_rxmode结构的ignore_offload_bitfield位。这种情况不需要设置rxmode结构的其他位。

## 8.5. 轮询模式驱动API
### 8.5.1. 概述
默认情况下，PMD的所有函数都是无锁的，多个逻辑核不会并行对相同目标调用这些函数。举例来说，两个逻辑核不能并行对同一端口的同一个队列调用PMD接收函数。当然，不同逻辑核可以对不同RX队列并行调用这个函数。上层应用需要保证这个规则。

如果需要，可以使用PMD的无锁函数实现锁保护的函数，从而在多个逻辑核上并行访问共享队列。

### 8.5.2. 通用报文表示
使用rte_mbuf结构表示报文，这是一个通用的metadata结构，包含了所有必要的信息。包括用于硬件offload功能的字段和状态位，比如IP头的checksum计算或者VLAN标签。

rte_mbuf包含了特定字段，用于表示网卡提供的offload功能。对于接收的报文，PMD的接收函数根据接收描述符的信息来填充rte_mbuf结构的大部分字段。对应的，对于传输的报文，PMD传输函数使用rte_mbuf结构的大部分字段来初始化传输描述符。

[Mbuf Library](https://github.com/gogodick/dpdk_prog_guide/blob/master/Text/7.md)详细的描述了mbuf结构体。

### 8.5.3. 以太网设备API
DPDK API Reference描述了以太网PMD提供的以太网设备API。

### 8.5.4. 扩展统计API
扩展统计API能够导出PMD支持的所有统计信息，包括这个设备特有的统计信息。每个统计信息有三个属性，名字，id和值：

名字：符合下面格式的可读的字符串  
id：唯一标识这个统计信息的整型数字  
value：统计的值是64位的无符号整型数据  

注意扩展统计标识是驱动指定的，所以不同端口的标识可能不同。API包括不同的rte_eth_xstats_\*()函数，允许应用灵活的获取统计信息。

#### 8.5.4.1. 可读的名字格式
为APi的客户提供了名字的格式。允许为需要的统计信息定制API。名字格式使用下划线分割字符串。格式如下：

* 方向
* 细节1
* 细节2
* 细节n
* 单位

常用统计xstat字符串的例子，符合上面的格式：

* rx_bytes
* rx_crc_errors
* tx_multicast_packets

这种格式虽然非常简单，可以更灵活的表示信息和读取信息。下面的例子说明了名字格式：rx_packets。这个例子的字符串分为两个成员。第一个成员rx表示统计是来自网卡的接收方向。第二个成员packets表示单位是报文。

一个更复杂的例子：tx_size_128_to_255_packets。在这个例子里，tx表示传输，size是第一个细节，128等等是更多细节，packets表示这是报文计数器。

metadata格式还有以下定义：

* 如果第一部分不是rx或者tx，统计和接收或者传输无关。
* 如果第二部分的第一个字母是q，后面是数字，这个统计是特定队列的统计

一个使用队列号的例子是：tx_q7_bytes，表示这是队列7的统计，表示这个队列传输的字节数。

#### 8.5.4.2. API设计
xstats的API使用名字，id和值实现对指定统计信息的高效查找。高效查找有两个含义：

* 在fast-path不对统计信息的名字执行字符串比较操作
* 允许只获取需要的统计信息

通过把统计信息的名字映射到唯一的id，满足了这些需求，id用来在fast-path作为查找的key。API允许应用申请id数组，从而PMD只执行要求的计算。期望的用法是，应用扫描统计的名字，保存需要统计信息的id。在fast-path，使用id获取统计信息的实际数据。

#### 8.5.4.3. API函数
API由少量函数组成，可以用来获取统计信息的数量，名字，ID和数据。

* rte_eth_xstats_get_names_by_id()：返回统计的名字。入参是NULL时，返回所有支持的统计个数。
* rte_eth_xstats_get_id_by_name()：查找xstat_name对应的统计信息ID。如果找到，返回id的值。
* rte_eth_xstats_get_by_id()：入参是id数组，返回对应统计的uint64_t数据数组。如果id数组为空，返回所有支持的统计数据。
#### 8.5.4.4. 应用的用法
设想一个应用需要获取丢弃报文的个数。如果没有丢弃报文，由于性能原因，应用不再读取任何其他统计。如果丢弃了报文，应用需要读取一系列统计信息。这些统计信息帮助应用决定下一步的操作。下面的代码片段显示了怎么使用xstats的API来实现这个目标。

第一步是获取全部统计名字并且列表：

```
struct rte_eth_xstat_name *xstats_names;
uint64_t *values;
int len, i;

/* Get number of stats */
len = rte_eth_xstats_get_names_by_id(port_id, NULL, NULL, 0);
if (len < 0) {
    printf("Cannot get xstats count\n");
    goto err;
}

xstats_names = malloc(sizeof(struct rte_eth_xstat_name) * len);
if (xstats_names == NULL) {
    printf("Cannot allocate memory for xstat names\n");
    goto err;
}

/* Retrieve xstats names, passing NULL for IDs to return all statistics */
if (len != rte_eth_xstats_get_names_by_id(port_id, xstats_names, NULL, len)) {
    printf("Cannot get xstat names\n");
    goto err;
}

values = malloc(sizeof(values) * len);
if (values == NULL) {
    printf("Cannot allocate memory for xstats\n");
    goto err;
}

/* Getting xstats values */
if (len != rte_eth_xstats_get_by_id(port_id, NULL, values, len)) {
    printf("Cannot get xstat values\n");
    goto err;
}

/* Print all xstats names and values */
for (i = 0; i < len; i++) {
    printf("%s: %"PRIu64"\n", xstats_names[i].name, values[i]);
}
```
应用可以得到PMD支持的全部统计信息的名字。应用可以决定需要的统计信息，通过查找名字保存这些统计信息的id：

```
uint64_t id;
uint64_t value;
const char *xstat_name = "rx_errors";

if(!rte_eth_xstats_get_id_by_name(port_id, xstat_name, &id)) {
    rte_eth_xstats_get_by_id(port_id, &id, &value, 1);
    printf("%s: %"PRIu64"\n", xstat_name, value);
}
else {
    printf("Cannot find xstats with a given name\n");
    goto err;
}
```
应用可以使用包含多个id的数组查找多个统计信息。减少了获取统计信息的函数调用开销，应用能够更简单的查找多个统计信息。

```
#define APP_NUM_STATS 4
/* application cached these ids previously; see above */
uint64_t ids_array[APP_NUM_STATS] = {3,4,7,21};
uint64_t value_array[APP_NUM_STATS];

/* Getting multiple xstats values from array of IDs */
rte_eth_xstats_get_by_id(port_id, ids_array, value_array, APP_NUM_STATS);

uint32_t i;
for(i = 0; i < APP_NUM_STATS; i++) {
    printf("%d: %"PRIu64"\n", ids_array[i], value_array[i]);
}
```
xstats的数组查找API允许应用创建多组统计信息，并且使用一个API调用查找这些ID对应的数据。应用达成了目标，监控一个统计信息（这个场景是rx_errors），如果显示报文被丢弃了，rte_eth_xstats_get_by_id函数可以使用ID数组获取一系列统计信息。

### 8.5.5. 网卡重置API
```
int rte_eth_dev_reset(uint16_t port_id);
```
Sometimes a port has to be reset passively. For example when a PF is reset, all its VFs should also be reset by the application to make them consistent with the PF. A DPDK application also can call this function to trigger a port reset. Normally, a DPDK application would invokes this function when an RTE_ETH_EVENT_INTR_RESET event is detected.

It is the duty of the PMD to trigger RTE_ETH_EVENT_INTR_RESET events and the application should register a callback function to handle these events. When a PMD needs to trigger a reset, it can trigger an RTE_ETH_EVENT_INTR_RESET event. On receiving an RTE_ETH_EVENT_INTR_RESET event, applications can handle it as follows: Stop working queues, stop calling Rx and Tx functions, and then call rte_eth_dev_reset(). For thread safety all these operations should be called from the same thread.

For example when PF is reset, the PF sends a message to notify VFs of this event and also trigger an interrupt to VFs. Then in the interrupt service routine the VFs detects this notification message and calls _rte_eth_dev_callback_process(dev, RTE_ETH_EVENT_INTR_RESET, NULL, NULL). This means that a PF reset triggers an RTE_ETH_EVENT_INTR_RESET event within VFs. The function _rte_eth_dev_callback_process() will call the registered callback function. The callback function can trigger the application to handle all operations the VF reset requires including stopping Rx/Tx queues and calling rte_eth_dev_reset().

The rte_eth_dev_reset() itself is a generic function which only does some hardware reset operations through calling dev_unint() and dev_init(), and itself does not handle synchronization, which is handled by application.

The PMD itself should not call rte_eth_dev_reset(). The PMD can trigger the application to handle reset event. It is duty of application to handle all synchronization before it calls rte_eth_dev_reset().
