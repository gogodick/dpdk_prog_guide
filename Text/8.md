
# 8. 轮询模式驱动
DPDK支持1G，10G，40G和半虚拟化的virtio轮询模式驱动。

轮询模式驱动(PMD)由API组成，这些API通过用户空间运行的BSD驱动提供，用于配置设备和对应的队列。此外，PMD不需要中断（除了链路状态改变中断），直接访问RX和TX的描述符，能够在用户应用快速的接收，处理和传输报文。本章描述了PMD的需求和总体设计原则，为以太网PMD提出了高层次架构和通用的外部API。

## 8.1. 需求和假设
DPDK环境对于报文处理应用支持两种模型，run-to-completion和pipe-line：

* 在run-to-completion模型里，通过API轮询指定端口的RX描述符ring。然后在同一个核上处理报文，通过传输API把报文放到端口的TX描述符ring。
* 在pipe-line模型里，1个核通过API轮询一个或者多个端口的RX描述符ring。报文通过ring传递到另一个核。另一个核继续处理报文，可能会通过传输API把报文放到端口的TX描述符ring。

在同步run-to-completion模型里，DPDK分配的逻辑核执行报文处理循环，包括以下步骤：

* 通过PMD接收API获取输入报文
* 每次处理一个接收的报文，直到转发
* 通过PMD传输API把报文发出去

相对的，在异步pipe-line模型里，一些逻辑核可能用来接收报文，而另一些逻辑核用来处理之前收到的报文。收到的报文在逻辑核之间通过ring交换。报文接收过程包括以下步骤：

* 通过PMD接收API获取输入报文
* 通过报文队列把报文发给处理lcore

报文处理包括以下步骤：

* 从报文队列获取收到的报文
* 处理收到的报文，直到重新传输

为了避免不必要的中断处理开销，执行环境不能使用任何异步通知机制。有必要时，应该尽可能使用ring来实现异步通知。

在多核环境中，避免锁竞争是关键的问题。为了解决这个问题，PMD的设计是尽可能使用每个核的私有资源。例如，如果PMD不支持DEV_TX_OFFLOAD_MT_LOCKFREE，PMD为每个port，每个核维护一个单独的传输队列。同样的，每个接收队列只分配给一个逻辑核（lcore）进行轮询。

为了和Non-Uniform Memory Access（NUMA）保持一致，内存管理的设计是为每个逻辑核在本地内存分配一个私有的内存池，从而最小化远端内存访问。报文内存池的配置应该考虑物理内存的架构，比如DIMM，通道和rank。应用必须保证内存池创建时使用合适的参数。参考[Mempool Library](https://github.com/gogodick/dpdk_prog_guide/blob/master/Text/6.md)。

## 8.2. 设计原则
以太网PMD的API和架构的设计遵循了以下原则。

PMD必须在上层应用帮助实现全局策略。相对的，网卡PMD函数不应该妨碍甚至阻止上层的全局策略。

举例来说，PMD的接收和传输函数都需要轮询大量的报文或者描述符。run-to-completion处理栈可以通过不同的全局循环策略来优化整体性能，比如：

* 每次接收，处理和传输一个报文，零碎的方式。
* 尽可能多的接收报文，然后处理全部报文，立即传输这些报文。
* 接收指定数量的报文，处理和积累这些收到的报文，最后把所有积累的报文发出去。

为了优化性能，必须考虑整体软件设计选择和纯软件优化技术，并且和底层硬件优化功能（CPU的cache属性，总线速度，网卡的PCI带宽等等）进行平衡。报文传输就是一个这样的例子，在优化突发网络报文的处理引擎时，要在软件和硬件之间权衡。对第一种情况，PMD只需要提供rte_eth_tx_one函数，每次在一个队列上传输一个报文。但是，PMD有效的实现了rte_eth_tx_burst函数，使得每个报文的驱动级别的传输代价最小化，使用了以下优化：

* 在多个报文间均分调用rte_eth_tx_one函数的未摊销代价。
* rte_eth_tx_burst函数利用了基于突发的硬件功能（预取数据到cache，使用网卡的头寄存器和尾寄存器），减少了处理每个报文的CPU周期，例如避免对ring传输描述符的没有必要的读访问，或者系统的使用匹配cache行边界和大小的指针数组。
* 使用基于突发的软件优化技术，省掉了其他情况下无法避免的操作，比如ring索引的回滚管理。

API也引入了基于突发的函数，PMD深入的使用了这些服务。特别是生成网卡ring的buffer分配器，能够一次分配和释放多个buffer。举例来说，mbuf_multiple_alloc返回一个指向rte_mbuf的指针数组，使得PMD的接收轮询函数能够在接收ring快速的安装多个描述符。

## 8.3. 逻辑核，内存和网卡队列关系
DPDK支持NUMA，当处理器的逻辑核和接口使用本地内存时，能够得到更好的性能。所以，为本地PCIE接口分配mbuf时，应该使用在本地内存创建的内存池。buffer应该在本地处理器使用，才能得到最好的性能，并且RX和TX的buffer描述符对应的mbuf应该使用在本地内存创建的内存池分配。

为了run-to-completion模型的性能更好，应该在本地内存而不是远端内存执行报文和数据的操作。对于pipe-line模型也一样，所有逻辑核都属于同样的处理器。

多个逻辑核绝不能共享接收队列和传输队列，因为需要全局锁从而影响性能。

如果PMD支持DEV_TX_OFFLOAD_MT_LOCKFREE，多个线程可以不使用软件锁，在同一个tx队列上同时调用rte_eth_tx_burst()。部分网卡支持这个PMD功能，在以下情况有用：

* 有些应用的lcore和TX队列的映射不是1：1，可以不使用spinlock。
* 对于eventdev，不需要用一个专用TX核传输，所有核都能传输报文，更加灵活

更多细节可以参考硬件能力DEV_TX_OFFLOAD_MT_LOCKFREE。

## 8.4. 设备识别和配置
### 8.4.1. 设备识别
每个网卡端口有唯一的PCI标识（总线号，设备号，功能号，BDF），是DPDK初始化执行的PCI探测和枚举函数分配的。基于PCI标识，网卡端口分配了两个其他的标识：

* 端口索引，PMD的API的所有函数使用这个索引来指定网卡端口。
* 端口名字，在控制台消息中指定端口，用于管理或者调试。为了便于使用，端口名字包含端口索引。
### 8.4.2. 设备配置
每个网卡端口配置包括以下操作：

* 分配PCI资源
* 重置设备（发送全局reset）到缺省状态
* 设置PHY和链路
* 初始化统计计数器

PMD的API必须提供函数支持开始和停止端口的多播功能，以及提供函数支持打开和关闭端口的混杂模式。

一些硬件offload功能必须在端口初始化阶段通过特定的参数配置。例如，Receive Side Scaling（RSS）和Data Center Bridging（DCB）就是这种情况。

### 8.4.3. 运行时修改配置
可以在运行时（不需要停止设备）打开或关闭的所有设备功能，不需要PMD的API提供专门的函数。

需要设备的PCI寄存器的映射地址，用来在驱动之外的特定函数实现配置这些功能。

为了这个目的，PMD的API提供了一个函数，能够提供设备有关的全部信息，可以用来在驱动之外配置指定的设备功能。包括了PCI供应商ID，PCI设备ID，PCI设备寄存器的映射地址，还有驱动的名字。

这个方式的主要优点是，可以自由的选择API来配置，开始和停止这种功能。

相应的例子可以参考testpmd应用，配置Intel的82576网卡和Intel的82599网卡的IEEE1588功能。

其他比如L3和L4的五元组的报文过滤功能，可以使用相同方式配置。以太网流控（pause帧）可以在独立端口配置。从testpmd源代码获取更多细节。另外，可以配置报文的mbuf，让网卡计算这个报文的L4（UDP，TCP和SCTP）校验和。参考硬件offload获取更多细节。

### 8.4.4. 传输队列的配置
每个传输队列的配置需要以下信息：

* 传输ring的描述符数量
* socket标识，指定NUMA架构中的合适DMA内存区域，用于分配传输ring
* 传输队列的Prefetch，Host和Write-Back阈值寄存器的值
* 最小传输报文的释放阈值（tx_free_thresh）。当传输报文使用的描述符数量超过这个阈值，网卡应该检查是否有回写描述符。配置TX队列时，0表示应该使用缺省值。tx_free_thresh的缺省值是32。这种方式保证了，网卡为队列处理了32个描述符以后，PMD才会搜索全部描述符。
* 最小RS位阈值。在设置传输描述符的Report Status（RS）位之前，使用的传输描述符的最小数量。注意这个参数只对Intel的10G网卡有效。如果从前一次设置RS位的描述符到这个报文的第一个描述符之间的描述符数量超过了传输RS位阈值（tx_rs_thresh），设置这个报文的最后一个描述符的RS位。简单的说，这个参数控制网卡把哪个传输描述符回写到主机内存。在配置TX队列时，0表示使用缺省值。tx_rs_thresh的缺省值是32。保证了在网卡回写最近使用的描述符之前，至少使用了32个描述符。这种方式节约了TX描述符回写使用的PCIE带宽。需要注意的是，当tx_rs_thresh大于1时，TX回写阈值（TX wthresh）应该设为0。参考Intel的82599的10G网卡数据表，获取更多细节。

tx_free_thresh和tx_rs_thresh必须满足以下限制：

* tx_rs_thresh必须大于0。
* tx_rs_thresh必须小于ring大小减2。
* tx_rs_thresh必须小于等于tx_free_thresh。
* tx_free_thresh必须大于0。
* tx_free_thresh必须小于ring大小减3。
* 为了优化性能，当tx_rs_thresh大于1时，TX wthresh必须设为0。

TX ring的一个描述必须作为哨兵使用，避免硬件冲突，这是最大阈值限制。

| 注意|
| :---|
|在端口初始化配置DCB操作时，传输队列和接收队列的数量必须配成128。|

### 8.4.5. 按需求释放TX的mbuf
很多驱动在传输报文后，不会立即释放mbuf到mempool或者本地cache。而是把mbuf留在TX的ring，等到超过tx_rs_thresh阈值后再批量释放，或者TX的ring需要槽位再释放mbuf。

应用可以使用rte_eth_tx_done_cleanup()的API通知驱动释放使用的mbuf。这个API通知驱动释放不再使用的mbuf，和tx_rs_thresh无关。以下是应用需要立即释放mbuf的两个场景：

* 报文需要传输到多个目的接口（二层的洪泛或者三层的组播）。一个选项是复制一份报文或者复制需要修改的报文头。另一个选项是传输报文，然后轮询rte_eth_tx_done_cleanup()这个API，直到报文的引用计数减少。然后同一个报文可以传输到下一个目的接口。应用仍然需要负责修改不同目的接口的报文，但是避免了复制报文。不管传输还是丢弃报文，API只关心接口是否还在使用mbuf。
* 一些应用需要多次运行，比如报文生成器。为了每次运行的性能和一致性，应用可能需要每次运行都重置到初始状态，释放所有mbuf到mempool。在这种情况下，应用可以对每个目的接口调用rte_eth_tx_done_cleanup()，通知释放所有mbuf。

在网卡驱动文档检查按需求释放TX的mbuf功能，确定驱动是否支持这个API。

### 8.4.6. 硬件offload
Depending on driver capabilities advertised by rte_eth_dev_info_get(), the PMD may support hardware offloading feature like checksumming, TCP segmentation, VLAN insertion or lockfree multithreaded TX burst on the same TX queue.

The support of these offload features implies the addition of dedicated status bit(s) and value field(s) into the rte_mbuf data structure, along with their appropriate handling by the receive/transmit functions exported by each PMD. The list of flags and their precise meaning is described in the mbuf API documentation and in the in [Mbuf Library](https://github.com/gogodick/dpdk_prog_guide/blob/master/Text/7.md), section “Meta Information”.

#### 8.4.6.1. Per-Port and Per-Queue Offloads
In the DPDK offload API, offloads are divided into per-port and per-queue offloads. The different offloads capabilities can be queried using rte_eth_dev_info_get(). Supported offloads can be either per-port or per-queue.

Offloads are enabled using the existing DEV_TX_OFFLOAD_* or DEV_RX_OFFLOAD_* flags. Per-port offload configuration is set using rte_eth_dev_configure. Per-queue offload configuration is set using rte_eth_rx_queue_setup and rte_eth_tx_queue_setup. To enable per-port offload, the offload should be set on both device configuration and queue setup. In case of a mixed configuration the queue setup shall return with an error. To enable per-queue offload, the offload can be set only on the queue setup. Offloads which are not enabled are disabled by default.

For an application to use the Tx offloads API it should set the ETH_TXQ_FLAGS_IGNORE flag in the txq_flags field located in rte_eth_txconf struct. In such cases it is not required to set other flags in txq_flags. For an application to use the Rx offloads API it should set the ignore_offload_bitfield bit in the rte_eth_rxmode struct. In such cases it is not required to set other bitfield offloads in the rxmode struct.

## 8.5. Poll Mode Driver API
### 8.5.1. Generalities
By default, all functions exported by a PMD are lock-free functions that are assumed not to be invoked in parallel on different logical cores to work on the same target object. For instance, a PMD receive function cannot be invoked in parallel on two logical cores to poll the same RX queue of the same port. Of course, this function can be invoked in parallel by different logical cores on different RX queues. It is the responsibility of the upper-level application to enforce this rule.

If needed, parallel accesses by multiple logical cores to shared queues can be explicitly protected by dedicated inline lock-aware functions built on top of their corresponding lock-free functions of the PMD API.

### 8.5.2. Generic Packet Representation
A packet is represented by an rte_mbuf structure, which is a generic metadata structure containing all necessary housekeeping information. This includes fields and status bits corresponding to offload hardware features, such as checksum computation of IP headers or VLAN tags.

The rte_mbuf data structure includes specific fields to represent, in a generic way, the offload features provided by network controllers. For an input packet, most fields of the rte_mbuf structure are filled in by the PMD receive function with the information contained in the receive descriptor. Conversely, for output packets, most fields of rte_mbuf structures are used by the PMD transmit function to initialize transmit descriptors.

The mbuf structure is fully described in the [Mbuf Library](https://github.com/gogodick/dpdk_prog_guide/blob/master/Text/7.md) chapter.

### 8.5.3. Ethernet Device API
The Ethernet device API exported by the Ethernet PMDs is described in the DPDK API Reference.

### 8.5.4. Extended Statistics API
The extended statistics API allows a PMD to expose all statistics that are available to it, including statistics that are unique to the device. Each statistic has three properties name, id and value:

name: A human readable string formatted by the scheme detailed below.
id: An integer that represents only that statistic.
value: A unsigned 64-bit integer that is the value of the statistic.
Note that extended statistic identifiers are driver-specific, and hence might not be the same for different ports. The API consists of various rte_eth_xstats_*() functions, and allows an application to be flexible in how it retrieves statistics.

#### 8.5.4.1. Scheme for Human Readable Names
A naming scheme exists for the strings exposed to clients of the API. This is to allow scraping of the API for statistics of interest. The naming scheme uses strings split by a single underscore _. The scheme is as follows:

* direction
* detail 1
* detail 2
* detail n
* unit

Examples of common statistics xstats strings, formatted to comply to the scheme proposed above:

* rx_bytes
* rx_crc_errors
* tx_multicast_packets

The scheme, although quite simple, allows flexibility in presenting and reading information from the statistic strings. The following example illustrates the naming scheme:rx_packets. In this example, the string is split into two components. The first component rx indicates that the statistic is associated with the receive side of the NIC. The second component packets indicates that the unit of measure is packets.

A more complicated example: tx_size_128_to_255_packets. In this example, tx indicates transmission, size is the first detail, 128 etc are more details, and packets indicates that this is a packet counter.

Some additions in the metadata scheme are as follows:

* If the first part does not match rx or tx, the statistic does not have an affinity with either receive of transmit.
* If the first letter of the second part is q and this q is followed by a number, this statistic is part of a specific queue.

An example where queue numbers are used is as follows: tx_q7_bytes which indicates this statistic applies to queue number 7, and represents the number of transmitted bytes on that queue.

#### 8.5.4.2. API Design
The xstats API uses the name, id, and value to allow performant lookup of specific statistics. Performant lookup means two things;

* No string comparisons with the name of the statistic in fast-path
* Allow requesting of only the statistics of interest

The API ensures these requirements are met by mapping the name of the statistic to a unique id, which is used as a key for lookup in the fast-path. The API allows applications to request an array of id values, so that the PMD only performs the required calculations. Expected usage is that the application scans the name of each statistic, and caches the id if it has an interest in that statistic. On the fast-path, the integer can be used to retrieve the actual value of the statistic that the id represents.

#### 8.5.4.3. API Functions
The API is built out of a small number of functions, which can be used to retrieve the number of statistics and the names, IDs and values of those statistics.

* rte_eth_xstats_get_names_by_id(): returns the names of the statistics. When given a NULL parameter the function returns the number of statistics that are available.
* rte_eth_xstats_get_id_by_name(): Searches for the statistic ID that matches xstat_name. If found, the id integer is set.
* rte_eth_xstats_get_by_id(): Fills in an array of uint64_t values with matching the provided ids array. If the ids array is NULL, it returns all statistics that are available.
#### 8.5.4.4. Application Usage
Imagine an application that wants to view the dropped packet count. If no packets are dropped, the application does not read any other metrics for performance reasons. If packets are dropped, the application has a particular set of statistics that it requests. This “set” of statistics allows the app to decide what next steps to perform. The following code-snippets show how the xstats API can be used to achieve this goal.

First step is to get all statistics names and list them:

```
struct rte_eth_xstat_name *xstats_names;
uint64_t *values;
int len, i;

/* Get number of stats */
len = rte_eth_xstats_get_names_by_id(port_id, NULL, NULL, 0);
if (len < 0) {
    printf("Cannot get xstats count\n");
    goto err;
}

xstats_names = malloc(sizeof(struct rte_eth_xstat_name) * len);
if (xstats_names == NULL) {
    printf("Cannot allocate memory for xstat names\n");
    goto err;
}

/* Retrieve xstats names, passing NULL for IDs to return all statistics */
if (len != rte_eth_xstats_get_names_by_id(port_id, xstats_names, NULL, len)) {
    printf("Cannot get xstat names\n");
    goto err;
}

values = malloc(sizeof(values) * len);
if (values == NULL) {
    printf("Cannot allocate memory for xstats\n");
    goto err;
}

/* Getting xstats values */
if (len != rte_eth_xstats_get_by_id(port_id, NULL, values, len)) {
    printf("Cannot get xstat values\n");
    goto err;
}

/* Print all xstats names and values */
for (i = 0; i < len; i++) {
    printf("%s: %"PRIu64"\n", xstats_names[i].name, values[i]);
}
```
The application has access to the names of all of the statistics that the PMD exposes. The application can decide which statistics are of interest, cache the ids of those statistics by looking up the name as follows:

```
uint64_t id;
uint64_t value;
const char *xstat_name = "rx_errors";

if(!rte_eth_xstats_get_id_by_name(port_id, xstat_name, &id)) {
    rte_eth_xstats_get_by_id(port_id, &id, &value, 1);
    printf("%s: %"PRIu64"\n", xstat_name, value);
}
else {
    printf("Cannot find xstats with a given name\n");
    goto err;
}
```
The API provides flexibility to the application so that it can look up multiple statistics using an array containing multiple id numbers. This reduces the function call overhead of retrieving statistics, and makes lookup of multiple statistics simpler for the application.

```
#define APP_NUM_STATS 4
/* application cached these ids previously; see above */
uint64_t ids_array[APP_NUM_STATS] = {3,4,7,21};
uint64_t value_array[APP_NUM_STATS];

/* Getting multiple xstats values from array of IDs */
rte_eth_xstats_get_by_id(port_id, ids_array, value_array, APP_NUM_STATS);

uint32_t i;
for(i = 0; i < APP_NUM_STATS; i++) {
    printf("%d: %"PRIu64"\n", ids_array[i], value_array[i]);
}
```
This array lookup API for xstats allows the application create multiple “groups” of statistics, and look up the values of those IDs using a single API call. As an end result, the application is able to achieve its goal of monitoring a single statistic (“rx_errors” in this case), and if that shows packets being dropped, it can easily retrieve a “set” of statistics using the IDs array parameter to rte_eth_xstats_get_by_id function.

### 8.5.5. NIC Reset API
```
int rte_eth_dev_reset(uint16_t port_id);
```
Sometimes a port has to be reset passively. For example when a PF is reset, all its VFs should also be reset by the application to make them consistent with the PF. A DPDK application also can call this function to trigger a port reset. Normally, a DPDK application would invokes this function when an RTE_ETH_EVENT_INTR_RESET event is detected.

It is the duty of the PMD to trigger RTE_ETH_EVENT_INTR_RESET events and the application should register a callback function to handle these events. When a PMD needs to trigger a reset, it can trigger an RTE_ETH_EVENT_INTR_RESET event. On receiving an RTE_ETH_EVENT_INTR_RESET event, applications can handle it as follows: Stop working queues, stop calling Rx and Tx functions, and then call rte_eth_dev_reset(). For thread safety all these operations should be called from the same thread.

For example when PF is reset, the PF sends a message to notify VFs of this event and also trigger an interrupt to VFs. Then in the interrupt service routine the VFs detects this notification message and calls _rte_eth_dev_callback_process(dev, RTE_ETH_EVENT_INTR_RESET, NULL, NULL). This means that a PF reset triggers an RTE_ETH_EVENT_INTR_RESET event within VFs. The function _rte_eth_dev_callback_process() will call the registered callback function. The callback function can trigger the application to handle all operations the VF reset requires including stopping Rx/Tx queues and calling rte_eth_dev_reset().

The rte_eth_dev_reset() itself is a generic function which only does some hardware reset operations through calling dev_unint() and dev_init(), and itself does not handle synchronization, which is handled by application.

The PMD itself should not call rte_eth_dev_reset(). The PMD can trigger the application to handle reset event. It is duty of application to handle all synchronization before it calls rte_eth_dev_reset().
