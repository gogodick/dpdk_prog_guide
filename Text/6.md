
# 6. Mempool库
内存池用于分配固定大小的对象。DPDK使用名字标识对象，并且使用mempool处理器保存空闲对象。缺省的mempool处理器是基于ring的。它还提供了其他的服务，比如基于核的对象cache，以及对齐机制，以确保对象被填充， 从而均匀的分布到DRAM或DDR3通道上。

[Mbuf Library](https://github.com/gogodick/dpdk_prog_guide/blob/master/Text/7.md)使用了这个库.

## 6.1. Cookies
在调试模式（使能了CONFIG_RTE_LIBRTE_MEMPOOL_DEBUG）下，把cookie加到分配的块的头部和尾部。分配的对象就包含了保护区域，帮助调试内存溢出问题。

## 6.2. Stats
在调试模式（使能了CONFIG_RTE_LIBRTE_MEMPOOL_DEBUG）下，mempool结构保存了分配内存和释放内存的统计数据。统计数据是基于核的，从而避免对统计计数器的并行访问。

## 6.3. 内存对齐限制
依赖于硬件内存的配置，通过在对象之间添加特定的填充，可以极大的提高性能。目标是保证每个对象的起始位置属于不同的内存通道，从而能够均匀的使用所有的内存通道。

在进行L3转发或者流分类时，这种方式会非常有效。只访问前64字节，所以通过把对象的起始地址分布到不同的通道能够提高性能。

任何DIMM的rank数量都是可访问DIMM完整数据位宽的独立DIMM集合的数量。因为rank共享了相同的数据路径，不能同时访问。DIMM的DRAM芯片的物理布局不一定和rank数量有关。

当运行应用时，EAL命令行选项提供了增加内存通道和rank的能力。

| 注意|
| :---|
|命令行必须为处理器指定内存通道数。|

图6.1和图6.2显示了不同的DIMM架构的内存对齐示例：

![Fig. 6.1 Two Channels and Quad-ranked DIMM Example](https://github.com/gogodick/dpdk_prog_guide/blob/master/Image/memory-management.svg)

在这种情况下，假设报文是16块64字节的数据，这个假设不能成立。

Intel 5520芯片组有三个内存通道，所以大多数情况下，对象之间不需要填充（除了大小为n x 3 x 64字节的对象）。

![Fig. 6.2 Three Channels and Two Dual-ranked DIMM Example](https://github.com/gogodick/dpdk_prog_guide/blob/master/Image/memory-management2.svg)

当创建新的池时，用户必须指定是否使用这个功能。

## 6.4. Local Cache
In terms of CPU usage, the cost of multiple cores accessing a memory pool’s ring of free buffers may be high since each access requires a compare-and-set (CAS) operation. To avoid having too many access requests to the memory pool’s ring, the memory pool allocator can maintain a per-core cache and do bulk requests to the memory pool’s ring, via the cache with many fewer locks on the actual memory pool structure. In this way, each core has full access to its own cache (with locks) of free objects and only when the cache fills does the core need to shuffle some of the free objects back to the pools ring or obtain more objects when the cache is empty.

While this may mean a number of buffers may sit idle on some core’s cache, the speed at which a core can access its own cache for a specific memory pool without locks provides performance gains.

The cache is composed of a small, per-core table of pointers and its length (used as a stack). This internal cache can be enabled or disabled at creation of the pool.

The maximum size of the cache is static and is defined at compilation time (CONFIG_RTE_MEMPOOL_CACHE_MAX_SIZE).

Fig. 6.3 shows a cache in operation.

![Fig. 6.3 A mempool in Memory with its Associated Ring](https://github.com/gogodick/dpdk_prog_guide/blob/master/Image/mempool.svg)

Alternatively to the internal default per-lcore local cache, an application can create and manage external caches through the rte_mempool_cache_create(), rte_mempool_cache_free() and rte_mempool_cache_flush() calls. These user-owned caches can be explicitly passed to rte_mempool_generic_put() and rte_mempool_generic_get(). The rte_mempool_default_cache() call returns the default internal cache if any. In contrast to the default caches, user-owned caches can be used by non-EAL threads too.

## 6.5. Mempool Handlers
This allows external memory subsystems, such as external hardware memory management systems and software based memory allocators, to be used with DPDK.

There are two aspects to a mempool handler.

* Adding the code for your new mempool operations (ops). This is achieved by adding a new mempool ops code, and using the MEMPOOL_REGISTER_OPS macro.
* Using the new API to call rte_mempool_create_empty() and rte_mempool_set_ops_byname() to create a new mempool and specifying which ops to use.
Several different mempool handlers may be used in the same application. A new mempool can be created by using the rte_mempool_create_empty() function, then using rte_mempool_set_ops_byname() to point the mempool to the relevant mempool handler callback (ops) structure.

Legacy applications may continue to use the old rte_mempool_create() API call, which uses a ring based mempool handler by default. These applications will need to be modified to use a new mempool handler.

For applications that use rte_pktmbuf_create(), there is a config setting (RTE_MBUF_DEFAULT_MEMPOOL_OPS) that allows the application to make use of an alternative mempool handler.

## 6.6. Use Cases
All allocations that require a high level of performance should use a pool-based memory allocator. Below are some examples:

* [Mbuf Library](https://github.com/gogodick/dpdk_prog_guide/blob/master/Text/7.md)
* [Environment Abstraction Layer](https://github.com/gogodick/dpdk_prog_guide/blob/master/Text/3.md), for logging service
* Any application that needs to allocate fixed-sized objects in the data plane and that will be continuously utilized by the system.
