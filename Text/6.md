
# 6. Mempool库
内存池用于分配固定大小的对象。DPDK使用名字标识对象，并且使用mempool处理器保存空闲对象。缺省的mempool处理器是基于ring的。它还提供了其他的服务，比如基于核的对象cache，以及对齐机制，以确保对象被填充， 从而均匀的分布到DRAM或DDR3通道上。

[Mbuf Library](https://github.com/gogodick/dpdk_prog_guide/blob/master/Text/7.md)使用了这个库.

## 6.1. Cookies
在调试模式（使能了CONFIG_RTE_LIBRTE_MEMPOOL_DEBUG）下，把cookie加到分配的块的头部和尾部。分配的对象就包含了保护区域，帮助调试内存溢出问题。

## 6.2. Stats
在调试模式（使能了CONFIG_RTE_LIBRTE_MEMPOOL_DEBUG）下，mempool结构保存了分配内存和释放内存的统计数据。统计数据是基于核的，从而避免对统计计数器的并行访问。

## 6.3. 内存对齐限制
依赖于硬件内存的配置，通过在对象之间添加特定的填充，可以极大的提高性能。目标是保证每个对象的起始位置属于不同的内存通道，从而能够均匀的使用所有的内存通道。

在进行L3转发或者流分类时，这种方式会非常有效。只访问前64字节，所以通过把对象的起始地址分布到不同的通道能够提高性能。

任何DIMM的rank数量都是可访问DIMM完整数据位宽的独立DIMM集合的数量。因为rank共享了相同的数据路径，不能同时访问。DIMM的DRAM芯片的物理布局不一定和rank数量有关。

当运行应用时，EAL命令行选项提供了增加内存通道和rank的能力。

| 注意|
| :---|
|命令行必须为处理器指定内存通道数。|

图6.1和图6.2显示了不同的DIMM架构的内存对齐示例：

![Fig. 6.1 Two Channels and Quad-ranked DIMM Example](https://github.com/gogodick/dpdk_prog_guide/blob/master/Image/memory-management.svg)

在这种情况下，假设报文是16块64字节的数据，这个假设不能成立。

Intel 5520芯片组有三个内存通道，所以大多数情况下，对象之间不需要填充（除了大小为n x 3 x 64字节的对象）。

![Fig. 6.2 Three Channels and Two Dual-ranked DIMM Example](https://github.com/gogodick/dpdk_prog_guide/blob/master/Image/memory-management2.svg)

当创建新的池时，用户必须指定是否使用这个功能。

## 6.4. 本地Cache
对于CPU使用率，因为需要使用compare-and-set（CAS）操作，多核访问内存池的空闲缓冲区的代价可能会很高。为了避免对内存池ring的访问太多，内存池分配器可以维护为每个核维护cache，并且向内存池ring发起批量操作，通过cache可以减少对内存池结构的锁操作。通过这种方式，每个核能够完全访问自己的空闲对象的cache（带锁），只有填充cache时，核需要把一些空闲对象释放回内存池ring，当cache空时，从内存池获取更多对象。

这意味着一定数量的buffer可能保存在某个核的cache里，一个核可以无锁的访问自己的内存池cache，带来了性能的提升。

cache是一个小型的基于核的表，包含了指针和长度，（作为栈使用）。在创建池时，可以打开或关闭cache机制。

cache的最大长度是固定的，在编译时决定（CONFIG_RTE_MEMPOOL_CACHE_MAX_SIZE）。

图6.3显示了工作中的cache

![Fig. 6.3 A mempool in Memory with its Associated Ring](https://github.com/gogodick/dpdk_prog_guide/blob/master/Image/mempool.svg)

除了内部缺省的基于核的本地cache，应用可以创建和管理外部cache，相应的API是rte_mempool_cache_create()，rte_mempool_cache_free()和rte_mempool_cache_flush()。这些用户的cache可以显式的传递给rte_mempool_generic_put()和rte_mempool_generic_get()。rte_mempool_default_cache()可以返回缺省的内部cache。和缺省cache不同，用户的cache可以在非EAL线程使用。

## 6.5. Mempool Handlers
This allows external memory subsystems, such as external hardware memory management systems and software based memory allocators, to be used with DPDK.

There are two aspects to a mempool handler.

* Adding the code for your new mempool operations (ops). This is achieved by adding a new mempool ops code, and using the MEMPOOL_REGISTER_OPS macro.
* Using the new API to call rte_mempool_create_empty() and rte_mempool_set_ops_byname() to create a new mempool and specifying which ops to use.
Several different mempool handlers may be used in the same application. A new mempool can be created by using the rte_mempool_create_empty() function, then using rte_mempool_set_ops_byname() to point the mempool to the relevant mempool handler callback (ops) structure.

Legacy applications may continue to use the old rte_mempool_create() API call, which uses a ring based mempool handler by default. These applications will need to be modified to use a new mempool handler.

For applications that use rte_pktmbuf_create(), there is a config setting (RTE_MBUF_DEFAULT_MEMPOOL_OPS) that allows the application to make use of an alternative mempool handler.

## 6.6. Use Cases
All allocations that require a high level of performance should use a pool-based memory allocator. Below are some examples:

* [Mbuf Library](https://github.com/gogodick/dpdk_prog_guide/blob/master/Text/7.md)
* [Environment Abstraction Layer](https://github.com/gogodick/dpdk_prog_guide/blob/master/Text/3.md), for logging service
* Any application that needs to allocate fixed-sized objects in the data plane and that will be continuously utilized by the system.
